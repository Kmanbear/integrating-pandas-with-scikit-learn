{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The ColumnTransformer\n",
    "\n",
    "The `ColumnTransformer` is a completely new meta-estimator released for scikit-learn version 0.20. It has greatly improved the transitioning from pandas to scikit-learn. The addition of the `ColumnTransformer` allows us to:\n",
    "\n",
    "* Apply different transformations to different columns of data\n",
    "* Use a pandas dataframe as input data\n",
    "* Create a pipeline connecting all of our steps\n",
    "\n",
    "## The previous workflow\n",
    "Before the `ColumnTransformer` existed, there was no direct way to apply different transformations to different columns of data. This was a major missing ability as string columns are processed differently than numeric columns. One of the most common frustrating scenarios was to have a pandas DataFrame consisting of both string and numeric columns with the desire to do machine learning on it in scikit-learn.\n",
    "\n",
    "### Common workarounds\n",
    "Because there was no available path in scikit-learn to prepare a pandas dataframe with a mix of string and numeric columns for machine learning, a number of workarounds were built.\n",
    "\n",
    "* pandas `get_dummies` function - is able to one-hot encode string columns (and ignore the numeric columns) but it is not an estimator, does not integrate with a pipeline, and cannot be used on unseen data to produce the same encoding.\n",
    "* scikit-learn's `MultiLabelBinarizer` - This actually does one-hot encoding for strings but was originally built for target variables and not input data. Importantly, you cannot apply it just a subset of the data.\n",
    "* The pandas [sklearn_pandas][1] library - The integration between pandas and scikit-learn was so bad, that an entire new package was built. It does provide an alternative solution to the integration problems, but ColumnTransformer solves these issues now.\n",
    "\n",
    "The `ColumnTransformer` paired with the upgraded `OneHotEncoder` makes the transition from pandas much easier, more robust, and gives us a single obvious path forward.\n",
    "\n",
    "[1]: https://github.com/scikit-learn-contrib/sklearn-pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing string and numeric columns separately\n",
    "String columns are processed differently than numeric columns and need separate pipelines to handle the transformations for each. For instance, we might want to impute missing values with a constant for string columns and the mean for numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hs = pd.read_csv('data/housing_sample.csv')\n",
    "hs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hs.pop('SalePrice').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `ColumnTransformer`\n",
    "The major purpose of the `ColumnTransformer` is to apply a transformation to a specific subset of the columns and not the entire input data which is done for all the other transformers. Let's say we want to only fill in missing values for the string columns. \n",
    "\n",
    "To get started, we must create a list of three-item tuples. The tuple items are the name of the transformer (as a string), the instantiated transformer, and the list of columns to apply the transformation to. The `ColumnTransformer` is built to be used with pandas DataFrames so you can pass in your DataFrame directly to it.\n",
    "\n",
    "Below, we create a list with a single three-item tuple to impute missing values to the 'Neighborhood' and 'Exterior1st' string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
    "string_cols = ['Neighborhood', 'Exterior1st']\n",
    "transformers = [('impute', si, string_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate the `ColumnTransformer` with our list of transformations and then fit and return the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers)\n",
    "X_filled = ct.fit_transform(hs)\n",
    "X_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns dropped - numpy array returned\n",
    "By default, all the columns not listed are dropped. Also, we are returned a numpy array and are no longer in pandas.\n",
    "\n",
    "### Keep the remaining columns\n",
    "The `remainder` parameter controls what happens to the other columns not specified in the list of transformers. This parameter is defaulted to the string 'drop'. Setting it to 'passthrough' will keep the other columns in the returned array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers, remainder='passthrough')\n",
    "ct.fit_transform(hs)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing the numeric columns with the mean\n",
    "The `ColumnTransformer` allows us to use different imputation strategies for different sets of columns. Below, we instantiate a second imputer and use it for the numeric columns. It's not necessary to use 'passthrough' as all the columns are named in the transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_si = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
    "numeric_si = SimpleImputer(strategy='mean')\n",
    "\n",
    "string_cols = ['Neighborhood', 'Exterior1st']\n",
    "numeric_cols = ['YearBuilt', 'LotFrontage', 'GrLivArea', 'GarageArea']\n",
    "\n",
    "transformers = [('impute_string', string_si, string_cols), \n",
    "                ('impute_numeic', numeric_si, numeric_cols)]\n",
    "\n",
    "ct = ColumnTransformer(transformers)\n",
    "ct.fit_transform(hs)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build separate pipelines\n",
    "If we want to apply multiple transformations to different subsets of the data then we will need to a build a pipeline for each section. Below, we impute missing values and one-hot encode the string columns and separately impute the missing values and standardize the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# string pipeline\n",
    "string_si = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "steps = [('impute', string_si), ('encode', ohe)]\n",
    "string_pipe = Pipeline(steps)\n",
    "\n",
    "# numeric pipeline\n",
    "numeric_si = SimpleImputer(strategy='mean')\n",
    "ss = StandardScaler()\n",
    "steps = [('si', numeric_si), ('standardize', ss)]\n",
    "numeric_pipe = Pipeline(steps)\n",
    "\n",
    "# columns\n",
    "string_cols = ['Neighborhood', 'Exterior1st']\n",
    "numeric_cols = ['YearBuilt', 'LotFrontage', 'GrLivArea', 'GarageArea']\n",
    "\n",
    "transformers = [('string', string_pipe, string_cols), \n",
    "                ('numeric', numeric_pipe, numeric_cols)]\n",
    "\n",
    "ct = ColumnTransformer(transformers)\n",
    "X_transformed = ct.fit_transform(hs)\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding machine learning\n",
    "We can create one last pipeline to do machine learning on the final transformed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from mymetrics import root_mean_squared_log_error\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "steps = [('transformers', ct), ('rfr', rfr)]\n",
    "final_pipe = Pipeline(steps)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "cross_val_score(final_pipe, hs, y, cv=kf, scoring=root_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid searching a pipeline of transformers\n",
    "To access a specific estimator in this pipeline of transformers, you must continually append the name of the transformer/pipeline followed by two underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = {'transformers__numeric__si__strategy': ['mean', 'median'],\n",
    "       'rfr__n_estimators': [50, 100], 'rfr__max_depth': range(2, 6)}\n",
    "gs = GridSearchCV(final_pipe, grid, cv=kf, scoring=root_mean_squared_log_error)\n",
    "gs.fit(hs, y)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from mymetrics import root_mean_squared_log_error\n",
    "\n",
    "# string pipeline\n",
    "string_si = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "steps = [('impute', string_si), ('encode', ohe)]\n",
    "string_pipe = Pipeline(steps)\n",
    "\n",
    "# numeric pipeline\n",
    "numeric_si = SimpleImputer(strategy='mean')\n",
    "ss = StandardScaler()\n",
    "steps = [('si', numeric_si), ('standardize', ss)]\n",
    "numeric_pipe = Pipeline(steps)\n",
    "\n",
    "# columns\n",
    "string_cols = ['Neighborhood', 'Exterior1st']\n",
    "numeric_cols = ['YearBuilt', 'LotFrontage', 'GrLivArea', 'GarageArea']\n",
    "\n",
    "transformers = [('string', string_pipe, string_cols), \n",
    "                ('numeric', numeric_pipe, numeric_cols)]\n",
    "\n",
    "ct = ColumnTransformer(transformers)\n",
    "rfr = RandomForestRegressor()\n",
    "steps = [('transformers', ct), ('rfr', rfr)]\n",
    "final_pipe = Pipeline(steps)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "grid = {'transformers__numeric__si__strategy': ['mean', 'median'],\n",
    "       'rfr__n_estimators': [50, 100], 'rfr__max_depth': range(2, 6)}\n",
    "gs = GridSearchCV(final_pipe, grid, cv=kf, scoring=root_mean_squared_log_error)\n",
    "gs.fit(hs, y)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Use the `ColumnTransformer` to build separate pipelines for string and numeric columns. Build a final pipeline that adds machine learning as the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
