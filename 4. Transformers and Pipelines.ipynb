{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transformers and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from mymetrics import root_mean_squared_log_error\n",
    "\n",
    "hs = pd.read_csv('data/housing_sample.csv')\n",
    "X = hs[['YearBuilt', 'GrLivArea', 'GarageArea']].values\n",
    "y = hs.pop('SalePrice').values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "grid = {'max_depth': range(2, 11), 'min_samples_split': [5, 10, 20, 50, 100]}\n",
    "gs = GridSearchCV(estimator=dtr, param_grid=grid, cv=kf, scoring=root_mean_squared_log_error)\n",
    "gs.fit(X, y)\n",
    "df_results = pd.DataFrame(gs.cv_results_)\n",
    "gs.best_params_\n",
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are a special class of estimators that transform either the input or output data independently. Transformations are applied to the data before the machine learning happens. Many transformers are found in the [preprocessing module][1].\n",
    "\n",
    "Although transformers don't do machine learning themselves, they still learn something from data and use the same three-step process - import, instantiate, fit. The `SimpleImputer` transformer from the `impute` module imputes (fills) missing data. Let's look at the number of missing values in each column.\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to use the `LotFrontage` column, we will need to impute the missing values (or drop the rows containing them entirely from the DataFrame). There are many strategies to imputing missing data. As the name suggests, the `SimpleImputer` only provides simple strategies that are set during instantiation. Set the `strategy` parameter to either 'mean', 'median', 'constant', or 'most_frequent'. If you select 'constant', you'll have to provide that constant with the `fill_value'` parameter. Let's complete the three-step process below by choosing to fill in missing values with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hs[['LotFrontage']].values\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer(strategy='mean')\n",
    "si.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit` doesn't fill the missing values\n",
    "Calling the `fit` method does not fill in the missing values. The mean of each column was learned which you can access with the `statistics_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the transformation with the `transform` method\n",
    "To actually fill the missing data (to be returned as a new copy), use the `transform` method after you have used the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filled = si.transform(X)\n",
    "X_filled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there are no more missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(X_filled).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data was not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip a step - fit and transform with `fit_transform`\n",
    "It is very common to call the `transform` method right after `fit`. scikit-learn provides all transformers the `fit_transform` method to both learn from the data and returned the new transformed dataset. The three-step process now becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer(strategy='mean')\n",
    "X_filled = si.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now do machine learning\n",
    "Once the missing values have been imputed, you can proceed to do machine learning in the same manner as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "cross_val_score(lr, X_filled, y, cv=kf, scoring=root_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pipeline to automate the process\n",
    "Notice how we had to assign the transformed data to a new variable name `X_filled`. If you had lots of transformations and wanted to do them in succession, this might start looking a bit cumbersome. Instead, you can use the `Pipeline` meta-estimator found in the `pipeline` module. To use, you must construct a list of all the transformers you'd want to apply to your dataset. If you want to do machine learning as well, you can include it as your last step.\n",
    "\n",
    "### Instantiate pipeline with a list of two-item tuples\n",
    "Specifically, the `Pipeline` estimator must be instantiated with a list of two-item tuples, where the first item in the tuple is a string naming that step in the pipeline and the second is the instantiated estimator. Let's create this list of two item tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = SimpleImputer(strategy='mean')\n",
    "lr = LinearRegression()\n",
    "\n",
    "step1 = ('impute', si)\n",
    "step2 = ('lin_reg', lr)\n",
    "\n",
    "steps = [step1, step2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three-step process with pipeline\n",
    "The same three-step process works with the pipeline. It imputes missing values and the learns the linear regression parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline(steps)\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipe, X, y, cv=kf, scoring=root_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid searching with the pipeline\n",
    "Completing a grid search is a little different with a pipeline. You can perform a search over all the hyperparameters in any part of the pipeline. To uniquely identify the parts of the pipeline, you need to use the name you provided to the part of the pipeline during instantiation and append it with two underscores. For instance, 'impute__strategy' is used to refer to the `strategy` hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hs[['YearBuilt', 'LotFrontage', 'GrLivArea', 'GarageArea']].values\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "si = SimpleImputer(strategy='mean')\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "step1 = ('impute', si)\n",
    "step2 = ('tree', dtr)\n",
    "steps = [step1, step2]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "grid = {'impute__strategy': ['mean', 'median'],\n",
    "        'tree__max_depth': range(2, 11),\n",
    "        'tree__min_samples_split': [5, 10, 50, 100]}\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=grid, cv=kf, scoring=root_mean_squared_log_error)\n",
    "gs.fit(X, y)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding another transformer\n",
    "Standardization is one of the most common transformation techniques. It subtracts the mean from each column and divides by the standard deviation. All the data is now scaled as the number of standard deviations away from the mean. Each resulting column will have a mean of 0 and standard deviation of 1. \n",
    "\n",
    "Standardization is necessary for machine learning models that depend on the relative size of column values. Penalized regression (Lasso and Ridge), k-nearest neighbors, and support vector machines all require input data to be standardized. Below, we use the `StandardScaler` transformer from the `preprocessing` module. We also use Ridge regression and optimize for the `alpha`, the size of the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "si = SimpleImputer(strategy='mean')\n",
    "ss = StandardScaler()\n",
    "ridge = Ridge()\n",
    "\n",
    "step1 = ('impute', si)\n",
    "step2 = ('standardize', ss)\n",
    "step3 = ('ridge', ridge)\n",
    "\n",
    "steps = [step1, step2, step3]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "grid = {'impute__strategy': ['mean', 'median'],\n",
    "        'ridge__alpha': np.logspace(-5, 5)}\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=grid, cv=kf, scoring=root_mean_squared_log_error)\n",
    "gs.fit(X, y)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Practice using some of the transformers separately and then together in a pipeline that ends with a machine learning algorithm. Finally, practice grid searching on different sections of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
